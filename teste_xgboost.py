import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

print("="*80)
print("TESTE BASELINE - MODELO COM FEATURES M√çNIMAS")
print("Objetivo: Verificar se o dataset TON-IoT tem separa√ß√£o trivial")
print("="*80)

# ========================================
# 1. CARREGAR DADOS
# ========================================
print("\n[1/5] Carregando dados...")
df = pd.read_csv('./ton-iot/Train_Test_datasets/Train_Test_Network_dataset/train_test_network.csv')
print(f"‚úì Dataset: {df.shape[0]:,} amostras, {df.shape[1]} colunas")
print(f"‚úì Distribui√ß√£o: {df['label'].value_counts(normalize=True).to_dict()}")

# ========================================
# 2. USAR APENAS FEATURES B√ÅSICAS DE FLUXO
# ========================================
print("\n[2/5] Selecionando APENAS features b√°sicas de fluxo...")

# Lista ULTRA RESTRITA: apenas estat√≠sticas de tr√°fego gen√©ricas
BASIC_FLOW_FEATURES = [
    'duration',      # Dura√ß√£o da conex√£o
    'src_bytes',     # Bytes enviados
    'dst_bytes',     # Bytes recebidos
    'src_pkts',      # Pacotes enviados
    'dst_pkts',      # Pacotes recebidos
    'src_port',      # Porta origem
    'dst_port',      # Porta destino
]

# Verificar quais existem
available_features = [f for f in BASIC_FLOW_FEATURES if f in df.columns]
missing_features = [f for f in BASIC_FLOW_FEATURES if f not in df.columns]

print(f"\n‚úì Features dispon√≠veis ({len(available_features)}):")
for f in available_features:
    print(f"   - {f}")

if missing_features:
    print(f"\n‚ö†Ô∏è  Features n√£o encontradas ({len(missing_features)}):")
    for f in missing_features:
        print(f"   - {f}")

# Criar dataset baseline
X = df[available_features].copy()
y = df['label'].astype(int)

print(f"\n‚úì Dataset baseline: {X.shape[1]} features")
print(f"‚úì Tipos de dados:")
print(X.dtypes)

# ========================================
# 3. AN√ÅLISE EXPLORAT√ìRIA
# ========================================
print("\n[3/5] An√°lise explorat√≥ria das features b√°sicas...")

# Estat√≠sticas por classe
print("\nüìä Estat√≠sticas por classe (primeiras 3 features):")
print("="*70)
for feature in available_features[:3]:
    print(f"\n{feature}:")
    print(f"  Benigno  (0): m√©dia={df[df['label']==0][feature].mean():.2f}, "
          f"std={df[df['label']==0][feature].std():.2f}")
    print(f"  Malicioso(1): m√©dia={df[df['label']==1][feature].mean():.2f}, "
          f"std={df[df['label']==1][feature].std():.2f}")

# ========================================
# 4. TREINAR MODELOS BASELINE
# ========================================
print("\n[4/5] Treinando modelos baseline...")

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚úì Train: {len(X_train):,} | Test: {len(X_test):,}")

# Pr√©-processamento
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- MODELO 1: Random Forest Simples ---
print("\nüå≤ Modelo 1: Random Forest (baseline)")
rf_model = RandomForestClassifier(
    n_estimators=50,
    max_depth=5,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_test_scaled)

rf_acc = accuracy_score(y_test, y_pred_rf)
rf_f1 = f1_score(y_test, y_pred_rf)

print(f"   Acur√°cia: {rf_acc:.4f}")
print(f"   F1-Score: {rf_f1:.4f}")

# --- MODELO 2: XGBoost Simplificado ---
print("\n‚ö° Modelo 2: XGBoost (simplificado)")
xgb_model = XGBClassifier(
    n_estimators=50,
    max_depth=3,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1
)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb = xgb_model.predict(X_test_scaled)

xgb_acc = accuracy_score(y_test, y_pred_xgb)
xgb_f1 = f1_score(y_test, y_pred_xgb)

print(f"   Acur√°cia: {xgb_acc:.4f}")
print(f"   F1-Score: {xgb_f1:.4f}")

# ========================================
# 5. DIAGN√ìSTICO
# ========================================
print("\n[5/5] DIAGN√ìSTICO FINAL...")
print("="*80)
print(f"{'Modelo':<20} {'Features':<15} {'Acur√°cia':<15} {'F1-Score':<15} {'Diagn√≥stico'}")
print("="*80)

# Diagn√≥stico Random Forest
rf_diag = "üö® TRIVIAL!" if rf_acc > 0.95 else ("‚úÖ RAZO√ÅVEL" if rf_acc > 0.85 else "‚ùå RUIM")
print(f"{'Random Forest':<20} {len(available_features):<15} {rf_acc:<15.4f} {rf_f1:<15.4f} {rf_diag}")

# Diagn√≥stico XGBoost
xgb_diag = "üö® TRIVIAL!" if xgb_acc > 0.95 else ("‚úÖ RAZO√ÅVEL" if xgb_acc > 0.85 else "‚ùå RUIM")
print(f"{'XGBoost':<20} {len(available_features):<15} {xgb_acc:<15.4f} {xgb_f1:<15.4f} {xgb_diag}")

print("="*80)

# Interpreta√ß√£o
print("\nüîç INTERPRETA√á√ÉO DOS RESULTADOS:\n")

if xgb_acc > 0.95:
    print("üö® ALERTA CR√çTICO: PROBLEMA TRIVIALMENTE SEPAR√ÅVEL!")
    print("\nCONCLUS√ÉO:")
    print("   O dataset TON-IoT tem separa√ß√£o quase perfeita entre classes")
    print("   mesmo usando APENAS {0} features b√°sicas de fluxo.".format(len(available_features)))
    print("\nISSO SIGNIFICA QUE:")
    print("   ‚úì N√ÉO h√° data leakage t√©cnico (IPs, timestamps, etc)")
    print("   ‚úì MAS os ataques t√™m padr√µes MUITO distintos no tr√°fego")
    print("   ‚úì O problema √© 'f√°cil demais' para ser realista")
    print("\nIMPLICA√á√ïES:")
    print("   ‚ö†Ô∏è  Modelo pode n√£o generalizar para ataques reais")
    print("   ‚ö†Ô∏è  Dataset pode ser sint√©tico ou muito controlado")
    print("   ‚ö†Ô∏è  Ataques podem ter sido gerados de forma artificial")
    
elif xgb_acc > 0.85:
    print("‚úÖ RESULTADO RAZO√ÅVEL")
    print(f"\nCom apenas {len(available_features)} features b√°sicas, alcan√ßamos {xgb_acc:.2%}.")
    print("Isso indica que:")
    print("   ‚úì O problema tem dificuldade moderada")
    print("   ‚úì Ataques t√™m padr√µes identific√°veis mas n√£o triviais")
    print("   ‚úì Modelo tem potencial de generaliza√ß√£o")
    
else:
    print("‚ùå PERFORMANCE BAIXA")
    print("\nIsso seria esperado se:")
    print("   ‚úì Ataques fossem muito similares a tr√°fego normal")
    print("   ‚úì Features b√°sicas n√£o fossem suficientes")
    print("   ‚ö†Ô∏è  Mas isso √© RARO em datasets de seguran√ßa")

# ========================================
# VISUALIZA√á√ïES
# ========================================
print("\nüìä Gerando visualiza√ß√µes...")

# 1. Matriz de Confus√£o
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

cm_rf = confusion_matrix(y_test, y_pred_rf)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)

sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0], 
            xticklabels=['Benigno', 'Malicioso'],
            yticklabels=['Benigno', 'Malicioso'])
axes[0].set_title(f'Random Forest (Acc: {rf_acc:.4f})')
axes[0].set_ylabel('True Label')
axes[0].set_xlabel('Predicted Label')

sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', ax=axes[1],
            xticklabels=['Benigno', 'Malicioso'],
            yticklabels=['Benigno', 'Malicioso'])
axes[1].set_title(f'XGBoost (Acc: {xgb_acc:.4f})')
axes[1].set_ylabel('True Label')
axes[1].set_xlabel('Predicted Label')

plt.tight_layout()
plt.savefig('baseline_confusion_matrix.png', dpi=150)
plt.show()

# 2. Feature Importance (XGBoost)
importances = xgb_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices], color='steelblue')
plt.xticks(range(len(importances)), 
           [available_features[i] for i in indices], 
           rotation=45, ha='right')
plt.title('Import√¢ncia das Features B√°sicas (XGBoost)', fontsize=14, fontweight='bold')
plt.ylabel('Import√¢ncia')
plt.xlabel('Feature')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('baseline_feature_importance.png', dpi=150)
plt.show()

print("\n‚úì Visualiza√ß√µes salvas:")
print("   - baseline_confusion_matrix.png")
print("   - baseline_feature_importance.png")

# ========================================
# CLASSIFICATION REPORT
# ========================================
print("\nüìã Classification Report Detalhado (XGBoost):")
print("="*70)
print(classification_report(y_test, y_pred_xgb, 
                          target_names=['Benigno', 'Malicioso'],
                          digits=4))

# ========================================
# RESUMO FINAL
# ========================================
print("\n" + "="*80)
print("RESUMO EXECUTIVO")
print("="*80)
print(f"‚úì Features utilizadas: {len(available_features)} (APENAS fluxo b√°sico)")
print(f"‚úì Melhor modelo: XGBoost")
print(f"‚úì Acur√°cia baseline: {xgb_acc:.4f}")
print(f"‚úì F1-Score baseline: {xgb_f1:.4f}")

if xgb_acc > 0.95:
    print(f"\nüö® CONCLUS√ÉO: Dataset TON-IoT √© TRIVIALMENTE SEPAR√ÅVEL")
    print(f"   Mesmo com features m√≠nimas, alcan√ßamos {xgb_acc:.2%}")
    print(f"   Isso explica por que seu modelo complexo chegou a 99.9%")
elif xgb_acc > 0.85:
    print(f"\n‚úÖ CONCLUS√ÉO: Problema tem dificuldade MODERADA")
    print(f"   Performance de {xgb_acc:.2%} √© razo√°vel com features b√°sicas")
else:
    print(f"\n‚ùì CONCLUS√ÉO: Performance ABAIXO do esperado")
    print(f"   Apenas {xgb_acc:.2%} - investigar qualidade dos dados")

print("="*80)