import pandas as pd
import numpy as np
import time
import warnings
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix, 
    classification_report, 
    ConfusionMatrixDisplay
)
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

warnings.filterwarnings('ignore')

print("="*80)
print("SISTEMA DE DETEC√á√ÉO DE AMEA√áAS DE REDE - VERS√ÉO CORRIGIDA")
print("="*80)

# ========================================
# 1. CARREGAMENTO DOS DADOS
# ========================================
print("\n[1/10] Carregando dados...")
try:
    df = pd.read_csv('./ton-iot/Train_Test_datasets/Train_Test_Network_dataset/train_test_network.csv')
    print(f"‚úì Dataset carregado: {df.shape[0]} amostras, {df.shape[1]} colunas")
    print(f"‚úì Distribui√ß√£o das classes:")
    print(df['label'].value_counts(normalize=True))
except Exception as e:
    print(f"‚úó Erro ao carregar dados: {e}")
    raise

# ========================================
# 2. AN√ÅLISE INICIAL
# ========================================
print("\n[2/10] An√°lise inicial do dataset...")
print(f"‚úì Colunas categ√≥ricas: {df.select_dtypes(include=['object']).columns.tolist()}")
print(f"‚úì Valores ausentes: {df.isnull().sum().sum()}")

# ========================================
# 3. SEPARA√á√ÉO DE FEATURES E TARGET
# ========================================
print("\n[3/10] Separando features e target...")

# Colunas a remover ANTES de qualquer processamento
COLS_TO_REMOVE = [
    # Target e tipo
    'label', 'type',
    
    # IPs (vazam informa√ß√£o!)
    'src_ip', 'dst_ip',
    
    # Colunas de assinatura (comportamento espec√≠fico)
    'dns_query', 'http_uri', 'ssl_subject', 'ssl_issuer', 
    'http_user_agent', 'weird_name', 'weird_addl', 'weird_notice', 
    'http_orig_mime_types', 'http_resp_mime_types',
    
    # Timestamps (se existirem)
    'ts', 'timestamp'
]

print(f"‚úì Removendo colunas problem√°ticas: {[c for c in COLS_TO_REMOVE if c in df.columns]}")

X = df.drop(columns=[col for col in COLS_TO_REMOVE if col in df.columns], errors='ignore')
y = df['label'].astype(int)

print(f"‚úì Features finais: {X.shape[1]} colunas")
print(f"‚úì Colunas restantes: {X.columns.tolist()[:10]}... (mostrando primeiras 10)")

# ========================================
# 4. TRAIN/VAL/TEST SPLIT
# ========================================
print("\n[4/10] Dividindo dados (60% train, 20% val, 20% test)...")

# Primeiro split: 80% treino+val, 20% teste
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y
)

# Segundo split: 75% do temp para treino (60% total), 25% para val (20% total)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, 
    test_size=0.25, 
    random_state=42, 
    stratify=y_temp
)

print(f"‚úì Train: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"‚úì Val:   {X_val.shape[0]:,} amostras ({X_val.shape[0]/len(X)*100:.1f}%)")
print(f"‚úì Test:  {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)")

# Verificar distribui√ß√£o de classes
print(f"\n‚úì Distribui√ß√£o Train: {y_train.value_counts(normalize=True).to_dict()}")

# ========================================
# 5. PR√â-PROCESSAMENTO
# ========================================
print("\n[5/10] Aplicando pr√©-processamento...")

# 5.1 Identificar colunas categ√≥ricas
categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()

print(f"‚úì Colunas num√©ricas: {len(numerical_cols)}")
print(f"‚úì Colunas categ√≥ricas: {len(categorical_cols)}")
if categorical_cols:
    print(f"  - {categorical_cols}")

# 5.2 ONE-HOT ENCODING (se houver categ√≥ricas)
if categorical_cols:
    print("\n  Aplicando One-Hot Encoding...")
    
    # Fit no treino
    X_train_encoded = pd.get_dummies(
        X_train, 
        columns=categorical_cols, 
        drop_first=True,  # Remove primeira categoria (evita multicolinearidade)
        dtype=int
    )
    
    # Transform em val e test
    X_val_encoded = pd.get_dummies(X_val, columns=categorical_cols, drop_first=True, dtype=int)
    X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True, dtype=int)
    
    # IMPORTANTE: Alinhar colunas (val e test podem ter categorias diferentes)
    train_cols = X_train_encoded.columns
    X_val_encoded = X_val_encoded.reindex(columns=train_cols, fill_value=0)
    X_test_encoded = X_test_encoded.reindex(columns=train_cols, fill_value=0)
    
    print(f"  ‚úì Colunas ap√≥s OHE: {len(train_cols)}")
else:
    X_train_encoded = X_train.copy()
    X_val_encoded = X_val.copy()
    X_test_encoded = X_test.copy()
    train_cols = X_train.columns

# 5.3 IMPUTA√á√ÉO (preencher valores ausentes)
print("\n  Aplicando imputa√ß√£o de valores ausentes...")
imputer = SimpleImputer(strategy='mean')

X_train_imputed = pd.DataFrame(
    imputer.fit_transform(X_train_encoded),
    columns=train_cols,
    index=X_train_encoded.index
)
X_val_imputed = pd.DataFrame(
    imputer.transform(X_val_encoded),
    columns=train_cols,
    index=X_val_encoded.index
)
X_test_imputed = pd.DataFrame(
    imputer.transform(X_test_encoded),
    columns=train_cols,
    index=X_test_encoded.index
)

print(f"  ‚úì Valores ausentes ap√≥s imputa√ß√£o: {X_train_imputed.isnull().sum().sum()}")

# 5.4 NORMALIZA√á√ÉO (StandardScaler)
print("\n  Aplicando normaliza√ß√£o (StandardScaler)...")
scaler = StandardScaler()

X_train_final = pd.DataFrame(
    scaler.fit_transform(X_train_imputed),
    columns=train_cols,
    index=X_train_imputed.index
)
X_val_final = pd.DataFrame(
    scaler.transform(X_val_imputed),
    columns=train_cols,
    index=X_val_imputed.index
)
X_test_final = pd.DataFrame(
    scaler.transform(X_test_imputed),
    columns=train_cols,
    index=X_test_imputed.index
)

print(f"  ‚úì Dados normalizados")
print(f"  ‚úì Shape final: {X_train_final.shape}")

# ========================================
# 6. TREINAMENTO DO MODELO
# ========================================
print("\n[6/10] Treinando modelo XGBoost...")

# Par√¢metros do modelo
xgb_params = {
    'n_estimators': 100,
    'max_depth': 6,
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'eval_metric': 'logloss',
    'random_state': 42,
    'n_jobs': -1
}

print(f"‚úì Par√¢metros: {xgb_params}")

xgb_model = XGBClassifier(**xgb_params)

# Treinar (sem early stopping para compatibilidade)
start_train = time.time()
xgb_model.fit(X_train_final, y_train)
end_train = time.time()

print(f"‚úì Treinamento conclu√≠do em {end_train - start_train:.2f}s")
print(f"‚úì N√∫mero de √°rvores treinadas: {xgb_model.n_estimators}")

# ========================================
# 7. AVALIA√á√ÉO NO CONJUNTO DE TREINO
# ========================================
print("\n[7/10] Avaliando no conjunto de TREINO...")
y_pred_train = xgb_model.predict(X_train_final)
train_metrics = {
    'accuracy': accuracy_score(y_train, y_pred_train),
    'precision': precision_score(y_train, y_pred_train),
    'recall': recall_score(y_train, y_pred_train),
    'f1': f1_score(y_train, y_pred_train)
}

print(f"‚úì Acur√°cia:  {train_metrics['accuracy']:.4f}")
print(f"‚úì Precision: {train_metrics['precision']:.4f}")
print(f"‚úì Recall:    {train_metrics['recall']:.4f}")
print(f"‚úì F1-Score:  {train_metrics['f1']:.4f}")

# ========================================
# 8. AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO
# ========================================
print("\n[8/10] Avaliando no conjunto de VALIDA√á√ÉO...")
y_pred_val = xgb_model.predict(X_val_final)
val_metrics = {
    'accuracy': accuracy_score(y_val, y_pred_val),
    'precision': precision_score(y_val, y_pred_val),
    'recall': recall_score(y_val, y_pred_val),
    'f1': f1_score(y_val, y_pred_val)
}

print(f"‚úì Acur√°cia:  {val_metrics['accuracy']:.4f}")
print(f"‚úì Precision: {val_metrics['precision']:.4f}")
print(f"‚úì Recall:    {val_metrics['recall']:.4f}")
print(f"‚úì F1-Score:  {val_metrics['f1']:.4f}")

# ========================================
# 9. AVALIA√á√ÉO NO CONJUNTO DE TESTE
# ========================================
print("\n[9/10] Avaliando no conjunto de TESTE...")
start_predict = time.time()
y_pred_test = xgb_model.predict(X_test_final)
end_predict = time.time()

test_metrics = {
    'accuracy': accuracy_score(y_test, y_pred_test),
    'precision': precision_score(y_test, y_pred_test),
    'recall': recall_score(y_test, y_pred_test),
    'f1': f1_score(y_test, y_pred_test)
}

print(f"‚úì Acur√°cia:  {test_metrics['accuracy']:.4f}")
print(f"‚úì Precision: {test_metrics['precision']:.4f}")
print(f"‚úì Recall:    {test_metrics['recall']:.4f}")
print(f"‚úì F1-Score:  {test_metrics['f1']:.4f}")
print(f"‚úì Tempo de infer√™ncia: {end_predict - start_predict:.4f}s para {len(X_test):,} amostras")

print("\n‚úì Classification Report Detalhado:")
print(classification_report(y_test, y_pred_test, target_names=['Benigno', 'Malicioso']))

# ========================================
# 10. VERIFICA√á√ÉO DE OVERFITTING
# ========================================
print("\n[10/10] Verifica√ß√£o de Overfitting...")
print("="*80)
print(f"{'M√©trica':<15} {'Train':<12} {'Val':<12} {'Test':<12} {'Diferen√ßa (Train-Test)'}")
print("="*80)

for metric in ['accuracy', 'precision', 'recall', 'f1']:
    diff = train_metrics[metric] - test_metrics[metric]
    status = "‚ö†Ô∏è" if diff > 0.05 else "‚úÖ"
    print(f"{metric.capitalize():<15} {train_metrics[metric]:.4f}      "
          f"{val_metrics[metric]:.4f}      {test_metrics[metric]:.4f}      "
          f"{diff:+.4f} {status}")

print("="*80)

# Diagn√≥stico
train_test_diff = train_metrics['accuracy'] - test_metrics['accuracy']
if train_test_diff > 0.05:
    print("\n‚ö†Ô∏è  ATEN√á√ÉO: Poss√≠vel OVERFITTING detectado!")
    print(f"   Diferen√ßa Train-Test: {train_test_diff:.4f} (> 0.05)")
    print("   Recomenda√ß√µes:")
    print("   - Reduzir max_depth")
    print("   - Aumentar min_child_weight")
    print("   - Reduzir n_estimators")
    print("   - Aumentar regulariza√ß√£o (reg_alpha, reg_lambda)")
elif train_metrics['accuracy'] < 0.85:
    print("\n‚ö†Ô∏è  ATEN√á√ÉO: Poss√≠vel UNDERFITTING detectado!")
    print(f"   Acur√°cia no treino muito baixa: {train_metrics['accuracy']:.4f}")
    print("   Recomenda√ß√µes:")
    print("   - Aumentar max_depth")
    print("   - Aumentar n_estimators")
    print("   - Adicionar mais features")
else:
    print("\n‚úÖ MODELO EST√Å GENERALIZANDO BEM!")
    print(f"   Diferen√ßa Train-Test: {train_test_diff:.4f} (< 0.05)")

# ========================================
# VISUALIZA√á√ïES
# ========================================
print("\nüìä Gerando visualiza√ß√µes...")

# 1. Matriz de Confus√£o
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

cm_val = confusion_matrix(y_val, y_pred_val)
cm_test = confusion_matrix(y_test, y_pred_test)

disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=['Benigno', 'Malicioso'])
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=['Benigno', 'Malicioso'])

disp_val.plot(ax=axes[0], cmap='Blues')
axes[0].set_title(f'Valida√ß√£o (Acc: {val_metrics["accuracy"]:.4f})')

disp_test.plot(ax=axes[1], cmap='Greens')
axes[1].set_title(f'Teste (Acc: {test_metrics["accuracy"]:.4f})')

plt.tight_layout()
plt.show()

# 2. Top 15 Features Mais Importantes
print("\nüìä Top 15 Features Mais Importantes:")
importances = xgb_model.feature_importances_
indices = np.argsort(importances)[::-1][:15]

print("="*60)
for i, idx in enumerate(indices, 1):
    print(f"{i:2d}. {train_cols[idx]:<40} {importances[idx]:.4f}")
print("="*60)

# Verificar se h√° features suspeitas
suspicious_features = [f for f in train_cols[indices[:15]] 
                      if any(x in str(f).lower() for x in ['ip_', 'mac_', 'addr_'])]
if suspicious_features:
    print(f"\n‚ö†Ô∏è  ALERTA: Features suspeitas encontradas: {suspicious_features}")
    print("   Essas features podem estar vazando informa√ß√£o!")
else:
    print("\n‚úÖ Nenhuma feature suspeita detectada nas top 15")

# Gr√°fico de import√¢ncia
plt.figure(figsize=(12, 6))
plt.bar(range(15), importances[indices], color='steelblue')
plt.xticks(range(15), [train_cols[i] for i in indices], rotation=45, ha='right')
plt.title('Top 15 Features Mais Importantes', fontsize=14, fontweight='bold')
plt.ylabel('Import√¢ncia', fontsize=12)
plt.xlabel('Feature', fontsize=12)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# 3. Compara√ß√£o de M√©tricas
metrics_names = list(train_metrics.keys())
train_values = [train_metrics[m] for m in metrics_names]
val_values = [val_metrics[m] for m in metrics_names]
test_values = [test_metrics[m] for m in metrics_names]

x = np.arange(len(metrics_names))
width = 0.25

fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(x - width, train_values, width, label='Train', color='#2ecc71')
ax.bar(x, val_values, width, label='Val', color='#3498db')
ax.bar(x + width, test_values, width, label='Test', color='#e74c3c')

ax.set_ylabel('Score', fontsize=12)
ax.set_title('Compara√ß√£o de M√©tricas (Train/Val/Test)', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels([m.capitalize() for m in metrics_names])
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0.7, 1.05])

plt.tight_layout()
plt.show()

# ========================================
# CROSS-VALIDATION (OPCIONAL)
# ========================================
print("\nüîÑ Executando Cross-Validation (5-fold)...")
cv_scores = cross_val_score(
    xgb_model, 
    X_train_final, 
    y_train, 
    cv=5, 
    scoring='accuracy',
    n_jobs=-1
)

print(f"‚úì CV Scores: {[f'{s:.4f}' for s in cv_scores]}")
print(f"‚úì M√©dia: {cv_scores.mean():.4f} (¬± {cv_scores.std():.4f})")

if cv_scores.std() > 0.05:
    print("‚ö†Ô∏è  Alta vari√¢ncia nos folds - modelo pode ser inst√°vel")
else:
    print("‚úÖ Baixa vari√¢ncia - modelo √© est√°vel")

# ========================================
# RESUMO FINAL
# ========================================
print("\n" + "="*80)
print("RESUMO FINAL")
print("="*80)
print(f"‚úì Dataset: {len(df):,} amostras")
print(f"‚úì Features utilizadas: {len(train_cols)}")
print(f"‚úì Tempo de treinamento: {end_train - start_train:.2f}s")
print(f"‚úì Tempo de infer√™ncia: {(end_predict - start_predict)/len(X_test)*1000:.2f}ms por amostra")
print(f"\n‚úì Acur√°cia Final (Test): {test_metrics['accuracy']:.4f}")
print(f"‚úì F1-Score Final (Test): {test_metrics['f1']:.4f}")
print(f"‚úì Status: {'‚úÖ APROVADO' if train_test_diff < 0.05 else '‚ö†Ô∏è  REVISAR'}")
print("="*80)